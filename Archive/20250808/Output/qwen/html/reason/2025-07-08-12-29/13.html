<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Export Google Site News</title>
    <style>
        body {
            font-family: Arial, sans-serif;
            line-height: 1.6;
            margin: 0;
            padding: 20px;
            background-color: #f0f0f0;
        }
        h1, h2, h3 {
            color: #333;
        }
        h1 {
            font-size: 2.5em;
            margin-bottom: 20px;
        }
        h2, h3 {
            font-size: 1.5em;
            margin-top: 40px;
        }
        p {
            margin-bottom: 20px;
        }
        .backstory {
            margin-top: 60px;
        }
        .backstory h2 {
            margin-top: 0;
        }
        .backstory p {
            margin-bottom: 20px;
        }
    </style>
</head>
<body>
    <header>
        <nav>
            <ul>
                <li><a href="#">Polibyte</a></li>
                <li><a href="#">About Me</a></li>
            </ul>
        </nav>
    </header>
    <main>
        <h1>Export news from google sites</h1>
        <p>Apr 27, 2012</p>
        <p>1 minute read</p>
        <p>I've added a script, <code>export_google_site_news</code>, to my catchall repository on github.</p>
        <p>For example, to download the news stories at <a href="https://sites.google.com/a/medinacommunityband.org/www/announcements">https://sites.google.com/a/medinacommunityband.org/www/announcements</a>, you would run</p>
        <pre><code>export_google_site_news 
https://sites.google.com/a/medinacommunityband.org/www/announcements</code></pre>
        <section class="backstory">
            <h2>The backstory</h2>
            <p>When Free It Athens moved our website from Google Sites to Drupal, we started from scratch rather than importing our old content. I realized on Wednesday that the news posts on the site were interesting historical information, yet I'd never archived them.</p>
            <p>First, I tried a recursive wget.</p>
            <pre><code>wget --no-parent --no-clobber 
</code></pre>
            <p>This failed to work because Google pointlessly used javascript rather than anchor tags to link between the news listing pages.</p>
            <p>Next I found and tried to use the google-sites-export tool from Google's Data Liberation Team, but I was never able to authenticate successfully from it.</p>
            <p>At this point I was worried I'd need to use a tool like Selenium to run the javascript, but then I realized the news listing pages took a single parameter to determine how far along in the pagination they were. It wouldn't take more than a C-style for loop to download them all.</p>
            <pre><code>for i in $(seq 0 10 120); do
    wget "" 
"_news.$i" 
done</code></pre>
            <p>After doing that, I looked at the first one and determined a pattern that would match the relative URLs of individual news stories. I then extracted all the URLs.</p>
            <pre><code>grep -h -o '/a/freeitathens.org/foo/news/[a-z0-9\-]+' news.* | sort -u > news_links</code></pre>
            <p>Once I had the list of URLs, it was simple to have wget download them all.</p>
            <pre><code>wget -i news_links -B https://sites.google.com</code></pre>
            <p>Since I didn't find any other guides to doing this, I decided to flesh out what I'd done into a simple tool and write about it here.</p>
        </section>
    </main>
</body>
</html>