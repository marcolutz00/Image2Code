<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Export news from google sites</title>
    <style>
        body {
            font-family: Arial, sans-serif;
            line-height: 1.6;
            color: #333;
            margin: 0;
            padding: 0;
            box-sizing: border-box;
        }
        .container {
            max-width: 800px;
            margin: 40px auto;
            padding: 0 20px;
        }
        header {
            display: flex;
            justify-content: space-between;
            align-items: center;
            margin-bottom: 40px;
        }
        header h1 {
            font-size: 1.2em;
            margin: 0;
        }
        header a {
            font-size: 0.9em;
            color: #666;
            text-decoration: none;
        }
        article h2 {
            font-size: 2em;
            margin: 0 0 10px;
        }
        .sub-header {
            font-size: 0.9em;
            color: #595959;
            margin-bottom: 20px;
        }
        .content {
            font-size: 1em;
            margin-bottom: 20px;
        }
        .content a {
            color: #a80a0a;
            text-decoration: underline;
        }
        .code-block {
            background-color: #f4f4f4;
            border: 1px solid #ddd;
            padding: 10px;
            font-family: monospace;
            overflow-x: auto;
            margin: 10px 0;
        }
        .section-title {
            font-weight: bold;
            font-size: 1.2em;
            margin: 20px 0 10px;
        }
    </style>
</head>
<body>
    <div class="container">
        <header>
            <h1>Polibyte</h1>
            <a href="#">About Me</a>
        </header>
        <main>
            <article>
                <h2>Export news from google sites</h2>
                <div class="sub-header">
                    Apr 27, 2012<br>
                    1 minute read
                </div>
                <div class="content">
                    IÃ¢ÂÂve added a script, <a href="#">export_google_site_news</a>, to my catchall repository on github.
                </div>
                <div class="content">
                    For example, to download the news stories at
                    <br>https://sites.google.com/a/medinacommunityband.org/www/announcements, you
                    <br>would run
                </div>
                <div class="code-block">
                    export_google_site_news <br>
                    https://sites.google.com/a/medinacommunityband.org/www <br>
                    announcements
                </div>
                <div class="section-title">The backstory</div>
                <div class="content">
                    When Free It Athens moved our website from Google Sites to Drupal, we started from scratch rather than importing our old content. I realized on Wednesday that the news posts on the site were interesting historical information, yet IÃ¢ÂÂd never archived them.
                </div>
                <div class="content">
                    First, I tried a recursive wget.
                </div>
                <div class="code-block">
                    wget -r --no-parent --no-clobber <br>
                    ""
                </div>
                <div class="content">
                    This failed to work because Google pointlessly used javascript rather than anchor tags to link between the news listing pages. 
                </div>
                <div class="content">
                    Next I found and tried to use the google-sites-export tool from GoogleÃ¢ÂÂs Data Liberation Team, but I was never able to authenticate successfully from it.
                </div>
                <div class="content">
                    At this point I was worried IÃ¢ÂÂd need to use a tool like Selenium to run the javascript, but then I realized the news listing pages took a single parameter to determine how far along in the pagination they were. It wouldnÃ¢ÂÂt take more than a C-style for loop to download them all.
                </div>
                <div class="code-block">
                    for i in $(seq 0 10 120); do<br>
                    &emsp;wget "" <br>
                    &emsp;"--Onews.$i"<br>
                    done
                </div>
                <div class="content">
                    After doing that, I looked at the first one and determined a pattern that would match the relative URLs of individual news stories. I then extracted all the URLs.
                </div>
                <div class="code-block">
                    grep -E -h -o '/a/freeitathens.org/foo/news/[a-z0-9\-_]+' news.* |<br>
                    sort -u > news_links
                </div>
                <div class="content">
                    Once I had the list of URLs, it was simple to have wget download them all.
                </div>
                <div class="code-block">
                    wget -i news_links -B https://sites.google.com
                </div>
                <div class="content">
                    Since I didnÃ¢ÂÂt find any other guides to doing this, I decided to flesh out what IÃ¢ÂÂd done into a simple tool and write about it here.
                </div>
            </article>
        </main>
    </div>
</body>
</html>