<!DOCTYPE html>
<html>
<head>
<meta charset="UTF-8">
<title>Polibyte</title>
<style>
body {
    font-family: sans-serif;
    color: #222;
    line-height: 1.6;
    margin: 0;
    padding: 20px;
    background-color: #f9f9f9;
}

h1 {
    font-size: 2.5em;
    font-weight: bold;
    margin-bottom: 0.5em;
}

p {
    margin-bottom: 1em;
}

a {
    color: #007bff;
    text-decoration: none;
}

a:hover {
    text-decoration: underline;
}

pre {
    background-color: #f0f0f0;
    padding: 10px;
    border-radius: 5px;
    overflow-x: auto;
}

.metadata {
    font-size: 0.8em;
    color: #666;
    margin-bottom: 1em;
}

.metadata a {
    color: #666;
}

.container {
    max-width: 800px;
    margin: 0 auto;
}

.more-info {
    text-align: right;
    font-size: 0.9em;
    color: #666;
}

.more-info a {
    color: #666;
}
</style>
</head>
<body>
<div class="container">
    <div class="more-info"><a href="#">More Info</a></div>
    <h1>Export news from google sites</h1>
    <p class="metadata">Apr 27, 2012<br>One min of reading</p>
    <p>I've put online a quick code snippet, <a href="#">export_google_site_news</a>, to my main repository located
        on github.</p>
    <p>To illustrate, for pulling the news posts from
        https://sites.google.com/a/medinacommunityband.org/www/announcements, you can call:</p>
    <pre>
export_google_site_news https://sites.google.com/a/medinacommunityband.org/www announcements
    </pre>
    <h2>Genesis of This</h2>
    <p>When Free It Athens migrated their site away from Google Sites over to Drupal, they went
        fresh, losing all the previous content. I just realized that news data on the old site was valuable
        historical content, and I hadn't saved it.</p>
    <p>Initially, I gave recursive wget a whirl.</p>
    <pre>
wget -r --no-parent --no-clobber     </pre>
    <p>This just flat failed as Google made the mistake of using javascript rather than anchor tags to
        connect news pages.</p>
    <p>I hunted down and tried Google's own google-sites-export from their Data Liberation Team, but
        could never authenticate correctly.</p>
    <p>I then feared I'd need Selenium to trigger the javascript, but then realized that the news pages
        took a single parameter to determine which paginated results to display. Looping would get
        them all.</p>
    <pre>
for i in $(seq 0 10 120); do
wget "" "-Onews.$i"
done
    </pre>
    <p>After getting the first one, I saw a pattern to match each news stories' relative paths. Then
        extracted out all those paths.</p>
    <pre>
grep -E -h -o '/a/freeitathens.org/foo/news/[a-z0-9\-]+' news.* |
sort -u > news_links
    </pre>
    <p>Once I had all the paths, it was easy to get wget to fetch them all.</p>
    <pre>
wget -i news_links -B https://sites.google.com
    </pre>
    <p>Since I found zero guides online, I tried writing up a simple script from the above, and am
        detailing it here.</p>
</div>
</body>
</html>