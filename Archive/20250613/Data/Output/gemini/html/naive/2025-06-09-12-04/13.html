<!DOCTYPE html>
<html>
<head>
<meta charset="UTF-8">
<title>Polibyte - Export news from google sites</title>
<style>
body {
    font-family: sans-serif;
    color: #333;
    line-height: 1.6;
    margin: 0;
    padding: 0;
    background-color: #f9f9f9;
}

.container {
    max-width: 800px;
    margin: 20px auto;
    padding: 20px;
    background-color: #fff;
    box-shadow: 0 0 5px rgba(0, 0, 0, 0.1);
}

h1 {
    font-size: 3em;
    margin-bottom: 20px;
}

p {
    margin-bottom: 15px;
}

a {
    color: #007BFF;
    text-decoration: none;
}

a:hover {
    text-decoration: underline;
}

pre {
    background-color: #f0f0f0;
    padding: 10px;
    overflow-x: auto;
    white-space: pre-wrap;
    word-wrap: break-word;
    font-family: monospace;
}

.metadata {
    font-size: 0.9em;
    color: #777;
    margin-bottom: 20px;
}

.header {
    display: flex;
    justify-content: space-between;
    align-items: center;
    padding: 10px 0;
}

.header a {
    color: #777;
}
</style>
</head>
<body>
<div class="container">
    <div class="header">
        <div>Polibyte</div>
        <div><a href="#">More Info</a></div>
    </div>

    <h1>Export news from google sites</h1>
    
    <div class="metadata">
        Apr 27, 2012<br>
        One min of reading
    </div>

    <p>I've put online a quick code snippet, <a href="#">export_google_site_news</a>, to my main repository located
    on github.</p>

    <p>To illustrate, for pulling the news posts from
    https://sites.google.com/a/medinacommunityband.org/www/announcements, you can call:</p>

    <pre>
export_google_site_news https://sites.google.com/a/medinacommunityband.org/wwwannouncements
    </pre>

    <h2>Genesis of This</h2>

    <p>When Free It Athens migrated their site away from Google Sites over to Drupal, they went
    fresh, losing all the previous content. I just realized that news data on the old site was valuable
    historical content, and I hadn't saved it.</p>

    <p>Initially, I gave recursive wget a whirl.</p>

    <pre>
wget -r --no-parent --no-clobber ""
    </pre>

    <p>This just flat failed as Google made the mistake of using javascript rather than anchor tags to
    connect news pages.</p>

    <p>I hunted down and tried Google's own google-sites-export from their Data Liberation Team, but
    could never authenticate correctly.</p>

    <p>I then feared I'd need Selenium to trigger the javascript, but then realized that the news pages
    took a single parameter to determine which paginated results to display. Looping would get
    them all.</p>

    <pre>
for i in $(seq 0 10 120); do
  wget "" "-Onews.$i"
done
    </pre>

    <p>After getting the first one, I saw a pattern to match each news stories' relative paths. Then
    extracted out all those paths.</p>

    <pre>
grep -E -h -o '/a/freeitathens.org/foo/news/[a-z0-9-]+' news.* |
sort -u > news_links
    </pre>

    <p>Once I had all the paths, it was easy to get wget to fetch them all.</p>

    <pre>
wget -i news_links -B https://sites.google.com
    </pre>

    <p>Since I found zero guides online, I tried writing up a simple script from the above, and am
    detailing it here.</p>
</div>
</body>
</html>