<!DOCTYPE html>
<html>
<head>
<meta charset="UTF-8">
<title>Polibyte</title>
<style>
body {
    font-family: sans-serif;
    color: #333;
    line-height: 1.6;
    margin: 0;
    padding: 0;
    background-color: #f8f8f8;
}

.container {
    width: 70%;
    margin: 20px auto;
    background-color: #fff;
    padding: 20px;
}

h1 {
    font-size: 3em;
    margin-bottom: 20px;
}

h2 {
    font-size: 1.8em;
    margin-top: 30px;
    margin-bottom: 15px;
}

p {
    margin-bottom: 15px;
}

pre {
    background-color: #f0f0f0;
    padding: 10px;
    overflow-x: auto;
    white-space: pre-wrap;
    word-wrap: break-word;
}

a {
    color: #007bff;
    text-decoration: none;
}

a:hover {
    text-decoration: underline;
}

.meta {
    color: #777;
    font-size: 0.9em;
    margin-bottom: 20px;
}

.more-info {
    float: right;
    color: #777;
    font-size: 0.9em;
}
</style>
</head>
<body>

<div class="container">
    <div class="more-info">More Info</div>
    <h1>Export news from google sites</h1>
    <div class="meta">
        Apr 27, 2012<br>
        One min of reading
    </div>

    <p>I've put online a quick code snippet, <a href="#">export_google_site_news</a>, to my main repository located
    on github.</p>
    <p>To illustrate, for pulling the news posts from
    https://sites.google.com/a/medinacommunityband.org/www/announcements, you can call:</p>

    <pre><code>export_google_site_news \
https://sites.google.com/a/medinacommunityband.org/www\
announcements</code></pre>

    <h2>Genesis of This</h2>
    <p>When Free It Athens migrated their site away from Google Sites over to Drupal, they went
    fresh, losing all the previous content. I just realized that news data on the old site was valuable
    historical content, and I hadn't saved it.</p>
    <p>Initially, I gave recursive wget a whirl.</p>

    <pre><code>wget -r --no-parent --no-clobber \
"""</code></pre>

    <p>This just flat failed as Google made the mistake of using javascript rather than anchor tags to
    connect news pages.</p>
    <p>I hunted down and tried Google's own google-sites-export from their Data Liberation Team, but
    could never authenticate correctly.</p>
    <p>I then feared I'd need Selenium to trigger the javascript, but then realized that the news pages
    took a single parameter to determine which paginated results to display. Looping would get
    them all.</p>

    <pre><code>for i in $(seq 0 10 120); do
wget "" \
"-Onews.$i"
done</code></pre>

    <p>After getting the first one, I saw a pattern to match each news stories' relative paths. Then
    extracted out all those paths.</p>

    <pre><code>grep -E -h -o '/a/freeitathens.org/foo/news/[a-z0-9\-]+' news.* |
sort -u > news_links</code></pre>

    <p>Once I had all the paths, it was easy to get wget to fetch them all.</p>

    <pre><code>wget -i news_links -B https://sites.google.com</code></pre>

    <p>Since I found zero guides online, I tried writing up a simple script from the above, and am
    detailing it here.</p>
</div>

</body>
</html>