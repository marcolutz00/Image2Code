<!DOCTYPE html>
<html>
<head>
<title>Polibyte</title>
<style>
body {
    font-family: sans-serif;
    margin: 0;
    padding: 20px;
    background-color: #f9f9f9;
    color: #333;
}

.container {
    max-width: 800px;
    margin: 0 auto;
}

h1 {
    font-size: 2.5em;
    line-height: 1.2em;
    margin-bottom: 20px;
    color: #222;
}

p {
    line-height: 1.6em;
    margin-bottom: 15px;
}

a {
    color: #0077cc;
    text-decoration: none;
}

a:hover {
    text-decoration: underline;
}

pre {
    background-color: #f0f0f0;
    padding: 10px;
    border-radius: 5px;
    overflow-x: auto;
    font-size: 0.9em;
}

.date {
    color: #777;
    font-size: 0.9em;
    margin-bottom: 10px;
}

.date em {
    font-style: italic;
}

.header {
    display: flex;
    justify-content: space-between;
    align-items: center;
    margin-bottom: 30px;
}

.header a {
    color: #777;
}
</style>
</head>
<body>
<div class="container">
    <div class="header">
        <div>Polibyte</div>
        <div><a href="#">More Info</a></div>
    </div>
    <h1>Export news from google sites</h1>
    <div class="date">Apr 27, 2012<br><em>One min of reading</em></div>
    <p>I've put online a quick code snippet, <a href="#">export_google_site_news</a>, to my main repository located
        on github.</p>
    <p>To illustrate, for pulling the news posts from
        https://sites.google.com/a/medinacommunityband.org/www/announcements, you can call:</p>
    <pre>
export_google_site_news \
https://sites.google.com/a/medinacommunityband.org/www\
announcements
    </pre>
    <h2>Genesis of This</h2>
    <p>When Free It Athens migrated their site away from Google Sites over to Drupal, they went
        fresh, losing all the previous content. I just realized that news data on the old site was valuable
        historical content, and I hadn't saved it.</p>
    <p>Initially, I gave recursive wget a whirl.</p>
    <pre>
wget -r --no-parent --no-clobber \
    </pre>
    <p>This just flat failed as Google made the mistake of using javascript rather than anchor tags to
        connect news pages.</p>
    <p>I hunted down and tried Google's own google-sites-export from their Data Liberation Team, but
        could never authenticate correctly.</p>
    <p>I then feared I'd need Selenium to trigger the javascript, but then realized that the news pages
        took a single parameter to determine which paginated results to display. Looping would get
        them all.</p>
    <pre>
for i in $(seq 0 10 120); do
  wget "" \
"-Onews.$i"
done
    </pre>
    <p>After getting the first one, I saw a pattern to match each news stories' relative paths. Then
        extracted out all those paths.</p>
    <pre>
grep -E -h -o '/a/freeitathens.org/foo/news/[a-z0-9-]+' news.* |
sort -u > news_links
    </pre>
    <p>Once I had all the paths, it was easy to get wget to fetch them all.</p>
    <pre>
wget -i news_links -B https://sites.google.com
    </pre>
    <p>Since I found zero guides online, I tried writing up a simple script from the above, and am
        detailing it here.</p>
</div>
</body>
</html>