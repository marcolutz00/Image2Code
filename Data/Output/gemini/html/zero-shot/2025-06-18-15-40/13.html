<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Export news from google sites</title>
    <style>
        body {
            font-family: sans-serif;
            line-height: 1.6;
            margin: 0;
            padding: 0;
            color: #333;
        }

        .container {
            max-width: 800px;
            margin: 20px auto;
            padding: 20px;
        }

        header {
            display: flex;
            justify-content: space-between;
            align-items: center;
            padding-bottom: 30px;
        }

        header a {
            color: #777;
            text-decoration: none;
        }

        h1 {
            font-size: 3em;
            line-height: 1.2;
            margin-bottom: 20px;
        }

        .meta {
            color: #777;
            font-size: 0.9em;
            margin-bottom: 30px;
        }

        a {
            color: #0077cc;
            text-decoration: none;
        }

        code {
            background-color: #f5f5f5;
            padding: 2px 5px;
            border-radius: 3px;
            font-family: monospace;
        }

        pre {
            background-color: #f5f5f5;
            padding: 10px;
            border-radius: 5px;
            overflow-x: auto;
            font-family: monospace;
        }
    </style>
</head>
<body>
    <div class="container">
        <header>
            <div>
                <h1>Polibyte</h1>
            </div>
            <div>
                <a href="#">About Me</a>
            </div>
        </header>

        <section>
            <h1>Export news from google sites</h1>
            <div class="meta">
                Apr 27, 2012
                <br>
                1 minute read
            </div>

            <p>I've added a script, <a href="#">export_google_site_news</a>, to my catchall repository on github.</p>

            <p>For example, to download the news stories at
                https://sites.google.com/a/medinacommunityband.org/www/announcements, you
                would run
            </p>

            <pre>
                <code>
export_google_site_news https://sites.google.com/a/medinacommunityband.org/www announcements
                </code>
            </pre>

            <h2>The backstory</h2>

            <p>When Free It Athens moved our website from Google Sites to Drupal, we started from
                scratch rather than importing our old content. I realized on Wednesday that the news
                posts on the site were interesting historical information, yet I'd never archived them.
            </p>

            <p>First, I tried a recursive wget.</p>

            <pre>
                <code>
wget -r --no-parent --no-clobber ...
                </code>
            </pre>

            <p>This failed to work because Google pointlessly used javascript rather than anchor tags
                to link between the news listing pages.
            </p>

            <p>Next I found and tried to use the google-sites-export tool from Google's Data
                Liberation Team, but I was never able to authenticate succesfully from it.
            </p>

            <p>At this point I was worried I'd need to use a tool like Selenium to run the javascript,
                but then I realized the news listing pages took a single paramater to determine how
                far along in the pagination they were. It wouldn't take more than a C-style for loop to
                download them all.
            </p>

            <pre>
                <code>
for i in $(seq 0 10 120); do
  wget ""   "-Onews.$i"
done
                </code>
            </pre>

            <p>After doing that, I looked at the first one and determined a pattern that would match
                the relative URLs of individual news stories. I then extracted all the URLS.
            </p>

            <pre>
                <code>
grep -E -h -o '/a/freeitathens.org/foo/news/[a-z0-9\-]+' news.* |
sort -u > news_links
                </code>
            </pre>

            <p>Once I had the list of URLs, it was simple to have wget download them all.</p>

            <pre>
                <code>
wget -i news_links -B https://sites.google.com
                </code>
            </pre>

            <p>Since I didn't find any other guides to doing this, I decided to flesh out what I'd done
                into a simple tool and write about it here.
            </p>
        </section>
    </div>
</body>
</html>