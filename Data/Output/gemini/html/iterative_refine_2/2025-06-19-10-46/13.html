<!DOCTYPE html>
<html>
<head>
<meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1.0">
<title>Export news from google sites</title>
<style>
body {
    font-family: sans-serif;
    line-height: 1.6;
    color: #212529;
    margin: 0;
    padding: 0;
}

.container {
    max-width: 800px;
    margin: 20px auto;
    padding: 20px;
}

h1 {
    font-size: 2.5rem;
    font-weight: bold;
    margin-bottom: 10px;
}

p {
    margin-bottom: 15px;
}

a {
    color: #007bff;
    text-decoration: none;
}

a:hover {
    text-decoration: underline;
}

pre {
    background-color: #f8f9fa;
    padding: 10px;
    overflow-x: auto;
    border: 1px solid #dee2e6;
    border-radius: 5px;
    font-family: monospace;
}

.metadata {
    color: #6c757d;
    font-size: 0.9rem;
    margin-bottom: 20px;
}

.metadata a {
    color: #6c757d;
}

.metadata a:hover {
    text-decoration: underline;
}

.header {
    display: flex;
    justify-content: space-between;
    align-items: center;
    padding: 10px 0;
}

.header a {
    color: #212529;
}

/* Added to fix contrast issues */
a:focus {
    outline: 2px solid #007bff;
    outline-offset: 2px;
}
</style>
</head>
<body>

<div class="container">
    <div class="header">
        <div>Polibyte</div>
        <div><a href="#">About Me</a></div>
    </div>

    <h1>Export news from google sites</h1>

    <div class="metadata">
        Apr 27, 2012<br>
        1 minute read
    </div>

    <p>I've added a script, <a href="#">export_google_site_news</a>, to my catchall repository on github.</p>

    <p>For example, to download the news stories at
        <a href="https://sites.google.com/a/medinacommunityband.org/www/announcements">https://sites.google.com/a/medinacommunityband.org/www/announcements</a>, you
        would run
    </p>

    <pre>
export_google_site_news https://sites.google.com/a/medinacommunityband.org/www announcements
    </pre>

    <h2>The backstory</h2>
    <p>When Free It Athens moved our website from Google Sites to Drupal, we started from
        scratch rather than importing our old content. I realized on Wednesday that the news
        posts on the site were interesting historical information, yet I'd never archived them.</p>

    <p>First, I tried a recursive wget.</p>

    <pre>
wget -r --no-parent --no-clobber ...
    </pre>

    <p>This failed to work because Google pointlessly used javascript rather than anchor tags
        to link between the news listing pages.</p>

    <p>Next I found and tried to use the google-sites-export tool from Google's Data
        Liberation Team, but I was never able to authenticate succesfully from it.</p>

    <p>At this point I was worried I'd need to use a tool like Selenium to run the javascript,
        but then I realized the news listing pages took a single paramater to determine how
        far along in the pagination they were. It wouldn't take more than a C-style for loop to
        download them all.</p>

    <pre>
for i in $(seq 0 10 120); do
  wget ""   "-Onews.$i"
done
    </pre>

    <p>After doing that, I looked at the first one and determined a pattern that would match
        the relative URLs of individual news stories. I then extracted all the URLS.</p>

    <pre>
grep -E -h -o '/a/freeitathens.org/foo/news/[a-z0-9-]+' news.* |
sort -u > news_links
    </pre>

    <p>Once I had the list of URLs, it was simple to have wget download them all.</p>

    <pre>
wget -i news_links -B https://sites.google.com
    </pre>

    <p>Since I didn't find any other guides to doing this, I decided to flesh out what I'd done
        into a simple tool and write about it here.</p>
</div>

</body>
</html>