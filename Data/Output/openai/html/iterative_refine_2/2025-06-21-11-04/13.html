<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Export News from Google Sites</title>
    <style>
        body {
            font-family: 'Georgia', serif;
            color: #333;
            margin: 0;
            padding: 20px;
            line-height: 1.6;
            background-color: #f9f9f9;
        }
        h1 {
            font-size: 2em;
            margin-top: 0;
        }
        .header {
            font-size: 1.8em;
            font-weight: bold;
        }
        a {
            color: #005ea6;
            text-decoration: underline;
        }
        p {
            margin-bottom: 20px;
            font-size: 1em;
        }
        pre {
            background-color: #f0f0f0;
            padding: 10px;
            border: 1px solid #ddd;
            overflow-x: auto;
        }
        .navbar {
            display: flex;
            justify-content: space-between;
            margin-bottom: 20px;
        }
        .navbar a {
            color: #005ea6;
            font-size: 1em;
        }
        .date {
            color: #999;
            font-size: 0.9em;
            font-style: italic;
        }
        .img-placeholder {
            width: 100%;
            height: auto;
            max-width: 600px;
            margin: 20px 0;
        }
    </style>
</head>
<body>
    <div class="navbar">
        <div class="header">Polibyte</div>
        <a href="#">About Me</a>
    </div>
    
    <h1>Export news from google sites</h1>
    
    <p class="date">Apr 27, 2012<br>1 minute read</p>
    
    <p>
        IÃ¢ÂÂve added a script, <a href="#">export_google_site_news</a>, to my catchall repository on github.
    </p>
    
    <p>
        For example, to download the news stories at 
        https://sites.google.com/a/medinacommunityband.org/www/announcements, you would run
    </p>
    
    <pre>
export_google_site_news     https://sites.google.com/a/medinacommunityband.org/www     announcements
    </pre>
    
    <h2>The backstory</h2>
    
    <p>
        When Free It Athens moved our website from Google Sites to Drupal, we started from 
        scratch rather than importing our old content. I realized on Wednesday that the news 
        posts on the site were interesting historical information, yet IÃ¢ÂÂd never archived them.
    </p>
    
    <p>
        First, I tried a recursive wget.
    </p>
    
    <pre>
wget -r --no-parent --no-clobber     ""
    </pre>
    
    <p>
        This failed to work because Google pointlessly used javascript rather than anchor tags 
        to link between the news listing pages.
    </p>
    
    <p>
        Next I found and tried to use the google-sites-export tool from GoogleÃ¢ÂÂs Data 
        Liberation Team, but I was never able to authenticate successfully from it.
    </p>
    
    <p>
        At this point I was worried IÃ¢ÂÂd need to use a tool like Selenium to run the javascript, 
        but then I realized the news listing pages took a single parameter to determine how 
        far along in the pagination they were. It wouldnÃ¢ÂÂt take more than a C-style for loop to 
        download them all.
    </p>
    
    <pre>
for i in $(seq 0 10 120); do
    wget ""     "-Onews.$i"
done
    </pre>
    
    <p>
        After doing that, I looked at the first one and determined a pattern that would match 
        the relative URLs of individual news stories. I then extracted all the URLs.
    </p>
    
    <pre>
grep -E -h -o '/a/freeitathens.org/foo/news/[a-z0-9\-_]+' news.* |
    sort -u > news_links
    </pre>
    
    <p>
        Once I had the list of URLs, it was simple to have wget download them all.
    </p>
    
    <pre>
wget -i news_links -B https://sites.google.com
    </pre>
    
    <p>
        Since I didnÃ¢ÂÂt find any other guides to doing this, I decided to flesh out what IÃ¢ÂÂd done 
        into a simple tool and write about it here.
    </p>
</body>
</html>