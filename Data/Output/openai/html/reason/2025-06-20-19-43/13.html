<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Export news from google sites</title>
    <style>
        body {
            font-family: 'Georgia', serif;
            margin: 0;
            padding: 0;
            box-sizing: border-box;
            background-color: #fff;
            color: #333;
            line-height: 1.6;
            padding: 20px;
            max-width: 700px;
            margin: auto;
        }
        header {
            display: flex;
            justify-content: space-between;
            margin-bottom: 20px;
        }
        header h1 {
            font-size: 22px;
            font-weight: normal;
        }
        a {
            color: #1a0dab;
            text-decoration: none;
        }
        h2 {
            font-size: 36px;
            font-weight: bold;
            margin: 20px 0 10px;
        }
        h3 {
            font-size: 24px;
            font-weight: normal;
            font-style: italic;
            margin-top: 0;
        }
        img {
            display: block;
            width: 100%;
            max-width: 100%;
            height: auto;
        }
        code {
            display: block;
            background-color: #f1f1f1;
            padding: 10px;
            border-radius: 5px;
            margin: 20px 0;
            color: #000;
            font-family: 'Courier New', Courier, monospace;
            overflow-x: auto;
        }
        .content {
            font-size: 18px;
        }
        .content p {
            margin: 0 0 20px;
        }
    </style>
</head>
<body>
    <header>
        <h1>Polibyte</h1>
        <nav><a href="#">About Me</a></nav>
    </header>
    <main>
        <article>
            <h2>Export news from google sites</h2>
            <h3>Apr 27, 2012<br>1 minute read</h3>
            <div class="content">
                <p>Iâve added a script, <a href="#">export_google_site_news</a>, to my catchall repository on github.</p>
                <p>For example, to download the news stories at
                    <wbr> <a href="#">https://sites.google.com/a/medinacommunityband.org/www/announcements</a>, you would run
                </p>
                <code>
export_google_site_news <br>
https://sites.google.com/a/medinacommunityband.org/www <br>
announcements
                </code>
                <h3>The backstory</h3>
                <p>When Free It Athens moved our website from Google Sites to Drupal, we started from scratch rather than importing our old content. I realized on Wednesday that the news posts on the site were interesting historical information, yet Iâd never archived them.</p>
                <p>First, I tried a recursive wget.</p>
                <code>
wget -r --no-parent --no-clobber <br>
""
                </code>
                <p>This failed to work because Google pointlessly used javascript rather than anchor tags to link between the news listing pages.</p>
                <p>Next I found and tried to use the google-sites-export tool from Googleâs Data Liberation Team, but I was never able to authenticate successfully from it.</p>
                <p>At this point I was worried Iâd need to use a tool like Selenium to run the javascript,
                but then I realized the news listing pages took a single parameter to determine how far along in
                the pagination they were. It wouldnât take more than a C-style for loop to download them all.</p>
                <code>
for i in $(seq 0 10 120); do
     wget "" 
     "--OnEws.$i"
 done
                </code>
                <p>After doing that, I looked at the first one and determined a pattern that would match the relative URLs of individual news stories. I then extracted all the URLs.</p>
                <code>
grep -E -h -o '/a/freeitathens.org/foo/news/[a-z0-9\-]+' news.* | 
sort -u > news_links
                </code>
                <p>Once I had the list of URLs, it was simple to have wget download them all.</p>
                <code>
wget -i news_links -B https://sites.google.com
                </code>
                <p>Since I didnât find any other guides to doing this, I decided to flesh out what Iâd done into a simple tool and write about it here.</p>
            </div>
        </article>
    </main>
</body>
</html>