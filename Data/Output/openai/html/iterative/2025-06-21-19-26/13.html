<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <style>
        body {
            font-family: Arial, sans-serif;
            color: #333;
            line-height: 1.6;
            max-width: 700px;
            margin: 40px auto;
            padding: 0 20px;
        }
        header {
            display: flex;
            justify-content: space-between;
            align-items: center;
            margin-bottom: 40px;
        }
        header h1 {
            margin: 0;
            font-size: 24px;
        }
        header a {
            font-size: 14px;
            color: #888;
            text-decoration: none;
        }
        h2 {
            font-size: 20px;
            margin: 0;
        }
        .meta {
            font-size: 14px;
            color: #555;
        }
        a {
            color: #3366CC;
            text-decoration: none;
        }
        .code-block {
            background-color: #f5f5f5;
            padding: 10px;
            border: 1px solid #ddd;
            border-radius: 3px;
            font-family: "Courier New", Courier, monospace;
            font-size: 14px;
            white-space: pre-wrap;
        }
        h3 {
            font-size: 16px;
            margin-top: 40px;
        }
    </style>
    <title>Export News from Google Sites</title>
</head>
<body>

<header>
    <h1>Polibyte</h1>
    <a href="#">About Me</a>
</header>

<h2>Export news from google sites</h2>
<div class="meta">
    Apr 27, 2012<br>
    <em>1 minute read</em>
</div>
<p>
    Iâve added a script, <a href="#">export_google_site_news</a>, to my catchall repository on github.
</p>
<p>
    For example, to download the news stories at https://sites.google.com/a/medinacommunityband.org/www/announcements, you would run
</p>
<div class="code-block">
export_google_site_news  <br>
https://sites.google.com/a/medinacommunityband.org/www  <br>
announcements
</div>

<h3>The backstory</h3>
<p>
    When Free It Athens moved our website from Google Sites to Drupal, we started from scratch rather than importing our old content. I realized on Wednesday that the news posts on the site were interesting historical information, yet Iâd never archived them.
</p>
<p>
    First, I tried a recursive wget.
</p>
<div class="code-block">
wget -r --no-parent --no-clobber  <br>
""
</div>
<p>
    This failed to work because Google pointlessly used javascript rather than anchor tags to link between the news listing pages.
</p>
<p>
    Next I found and tried to use the google-sites-export tool from Googleâs Data Liberation Team, but I was never able to authenticate successfully from it.
</p>
<p>
    At this point I was worried Iâd need to use a tool like Selenium to run the javascript, but then I realized the news listing pages took a single parameter to determine how far along in the pagination they were. It wouldnât take more than a C-style for loop to download them all.
</p>
<div class="code-block">
for i in $(seq 0 10 120); do <br>
&nbsp;&nbsp;wget ""  <br>
&nbsp;&nbsp;"--Onews.$i" <br>
done
</div>
<p>
    After doing that, I looked at the first one and determined a pattern that would match the relative URLs of individual news stories. I then extracted all the URLs.
</p>
<div class="code-block">
grep -E -h -o '/a/freeitathens.org/foo/news/[a-z0-9\-_]+' news.* | <br>
sort -u > news_links
</div>
<p>
    Once I had the list of URLs, it was simple to have wget download them all.
</p>
<div class="code-block">
wget -i news_links -B https://sites.google.com
</div>
<p>
    Since I didnât find any other guides to doing this, I decided to flesh out what Iâd done into a simple tool and write about it here.
</p>

</body>
</html>